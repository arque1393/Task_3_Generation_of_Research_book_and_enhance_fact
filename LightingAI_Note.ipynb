{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture \n",
    "!pip install trandformers huggingface_hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try meta-llama/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc95588d38f419fb00334ce1ee7408c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Generate Research 10 Book name on siblings murders\n",
      "\n",
      "1. \"The Deadly Bond: A Psychological Examination of Sibling Murders\"\n",
      "2. \"Sibling Serial Killers: An Exploration of the Dark Side of Family Ties\"\n",
      "3. \"Murderous Brothers: Understanding the Unholy Alliance of Sibling Killers\"\n",
      "4. \"The Family Secret: Uncovering the Truth Behind Sibling Murders\"\n",
      "5. \"Sibling Homicide: A Forensic Analysis of the Motives and Methods of Brothers Who Kill\"\n",
      "6. \"The Fatal Attraction: Why Brothers Turn to Murder Against Each Other\"\n",
      "7. \"The Shadow of Family: Unraveling the Mystery of Sibling Murders\"\n",
      "8. \"The Devil's Disciples: A Study of the Psychology of Sibling Killers\"\n",
      "9. \"Brotherly Betrayal: The Hidden Truth Behind Sibling Murders\"\n",
      "10. \"The Family Trap: Understanding the Complex Relationships That Lead to Sibling Homicide\"\n",
      "\n",
      "These book titles are meant to be thought-provoking and attention-grabbing, while also accurately reflecting the focus of the book. Each title explores a different aspect of sibling murders, from the psychological dynamics of the killers to the hidden truths behind these heinous crimes. By using words like \"deadly,\" \"dark,\" \"unholy,\" and \"fatal,\" these titles aim to convey the gravity and complexity of the topic, while also hinting at the fascinating insights and revelations that the book has to offer.</s>\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch import cuda\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "query = \"siblings murders\"\n",
    "input_text = f\"Generate Research 10 Book name on {query}\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhance Fact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Generate few facts on siblings murders.\n",
      "1. According to FBI crime statistics, sibling homicide accounts for only 2% of all murder cases in the United States.\n",
      "2. The majority of sibling murders occur between siblings who are close in age and are committed by individuals between the ages of 15 and 24.\n",
      "3. In many cases, sibling murders are the result of a long-standing conflict or abuse within the family.\n",
      "4. Sibling murders are more likely to occur in families with a history of domestic violence or child abuse.\n",
      "5. According to a study published in the Journal of Forensic Psychiatry and Psychology, siblings who commit murder are more likely to have a history of psychiatric disorders, such as schizophrenia or bipolar disorder, than non-sibling murderers.\n",
      "6. Sibling murderers often have a history of substance abuse and may use drugs or alcohol as a means of escalating violence within the family.\n",
      "7. In some cases, sibling murders may be the result of a planned and premeditated act, while in other cases, they may be the result of a sudden and intense conflict.\n",
      "8. The aftermath of a sibling murder can be particularly devastating for family members, who may experience a range of emotions, including grief, guilt, and anger.\n",
      "9. In some cases, sibling murders may be the result of a larger societal problem, such as poverty, lack of access to mental health services, or a culture of violence.\n",
      "10. According to the National Institute of Justice, sibling murder is a relatively rare form of violence, accounting for only 1% of all homicides in the United States.\n",
      "\n",
      "It is important to note that these statistics are based on averages and may not reflect the specific circumstances of every sibling murder case. Each case is unique and may involve a complex interplay of factors that contribute to the violence.</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"siblings murders\"\n",
    "input_text = f\"Generate few facts on {query}\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Interface With Grdio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install Gradio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a9cbc23f894d6e87caf757973dc57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch import cuda\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "def fact_enhancement(query):\n",
    "    input_text = f\"Generate Research 10 Book name on {query}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model.generate(input_ids)\n",
    "    return(tokenizer.decode(outputs[0]))\n",
    "\n",
    "def book_name_generation(query):\n",
    "    input_text = f\"Generate Research 10 Book name on {query}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model.generate(input_ids)\n",
    "    return(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "Running on public URL: https://d3b712b80d8bb34a45.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d3b712b80d8bb34a45.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "def generate_output(input_text, task_name):\n",
    "    if task_name == 'book_name_generation' :\n",
    "        return book_name_generation(input_text)\n",
    "    if task_name == 'fact_enhancement':\n",
    "        return fact_enhancement(input_text)\n",
    "\n",
    "# Define the options for the dropdown\n",
    "options = [\"book_name_generation\", \"fact_enhancement\"]\n",
    "# Create a dropdown input\n",
    "dropdown_input = gr.Dropdown(choices=options, label=\"Select an option\")\n",
    "input_text = gr.Textbox(lines=3, label=\"Enter your Topic here:\")\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=generate_output, \n",
    "    inputs=[input_text,dropdown_input],\n",
    "    outputs=\"text\")\n",
    "interface.launch(share=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
